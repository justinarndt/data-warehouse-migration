# ðŸ—ï¸ JD Edwards Data Warehouse Modernization

[![Azure](https://img.shields.io/badge/Azure-Data_Factory-0078D4?style=for-the-badge&logo=microsoftazure&logoColor=white)](https://azure.microsoft.com/en-us/products/data-factory)
[![Python](https://img.shields.io/badge/Python-3.11+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![SQL](https://img.shields.io/badge/T--SQL-Azure_SQL-CC2927?style=for-the-badge&logo=microsoftsqlserver&logoColor=white)](https://azure.microsoft.com/en-us/products/azure-sql/database/)
[![CI](https://img.shields.io/badge/CI-GitHub_Actions-2088FF?style=for-the-badge&logo=githubactions&logoColor=white)](https://github.com/features/actions)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)

> **Enterprise-grade data pipeline** migrating legacy JD Edwards EnterpriseOne ERP data into a modern Azure Cloud Data Warehouse using the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold).

---

## ðŸ“ Architecture

```mermaid
flowchart LR
    subgraph Source ["JD Edwards ERP"]
        F0101["F0101 â€” Address Book"]
        F4211["F4211 â€” Sales Orders"]
    end

    subgraph Bronze ["Bronze â€” ADLS Gen2"]
        B["CSV â†’ Parquet\nSnappy Compression\nTime Partitioned"]
    end

    subgraph Silver ["Silver â€” ADF Data Flows"]
        S1["Julian Date â†’ ISO Date\nImplicit Decimals â†’ Float"]
        S2["SCD Type 2\nSHA256 Change Detection"]
    end

    subgraph Gold ["Gold â€” Azure SQL"]
        G1["Dim_Date"]
        G2["Dim_Customer"]
        G3["Fact_Sales"]
    end

    subgraph Analytics ["Power BI / Tableau"]
        BI["Dashboards\n& Reports"]
    end

    F0101 --> B
    F4211 --> B
    B --> S1
    S1 --> S2
    S2 --> G1 & G2 & G3
    G3 --> BI

    style Source fill:#2d2d2d,color:#fff,stroke:#555
    style Bronze fill:#cd7f32,color:#fff,stroke:#a0642a
    style Silver fill:#b0b0b0,color:#000,stroke:#888
    style Gold fill:#daa520,color:#000,stroke:#b8860b
    style Analytics fill:#0078d4,color:#fff,stroke:#005a9e
```

> **Orchestration:** Azure Data Factory  â€¢  **Security:** Key Vault + Managed Identity  â€¢  **Monitoring:** Logic Apps Alerts  â€¢  **Budget:** < $200 Azure Credits

---

## ðŸŽ¯ Problem Statement

JD Edwards EnterpriseOne stores data using **legacy conventions** that break modern analytics tools:

| Challenge | JDE Format | Example Raw Value | Correct Value |
|-----------|-----------|-------------------|---------------|
| **Julian Dates** | `CYYDDD` | `123001` | `2023-01-01` |
| **Implicit Decimals** | Integer (Ã·100) | `1050` | `$10.50` |
| **Cryptic Columns** | Abbreviated codes | `SDAN8`, `SDAEXP` | Customer ID, Extended Price |

If these are not decoded, **revenue reports are overstated by 100Ã—** and dates appear as nonsensical values like "Year 123."

---

## âœ¨ Key Features

- **Medallion Architecture** â€” Bronze (raw), Silver (cleansed), Gold (star schema) for full auditability
- **SCD Type 2 Tracking** â€” Historical changes to customers preserved using SHA256 hash comparison
- **Metadata-Driven Ingestion** â€” Configuration-based pipeline; zero hardcoded paths
- **Point-in-Time Joins** â€” Sales attributed to the *historical* customer state at time of order
- **Logic Apps Alerting** â€” Real-time HTML email notifications on pipeline failures
- **Cost Optimized** â€” Full platform runs under **$200/month** using Basic DTU + ephemeral Spark
- **CI/CD Ready** â€” GitHub Actions validates Python, SQL, and ADF JSON on every push

---

## ðŸ“ Repository Structure

```
data-warehouse-migration/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml                  # GitHub Actions: lint Python, validate SQL & JSON
â”œâ”€â”€ adf/
â”‚   â”œâ”€â”€ dataflow/
â”‚   â”‚   â”œâ”€â”€ DF_Clean_JDE.json       # Julian date + decimal conversion logic
â”‚   â”‚   â””â”€â”€ DF_SCD2_Customer.json   # Slowly Changing Dimensions Type 2
â”‚   â”œâ”€â”€ dataset/
â”‚   â”‚   â”œâ”€â”€ ds_bronze_csv.json      # Parameterized CSV source
â”‚   â”‚   â”œâ”€â”€ ds_bronze_parquet.json  # Time-partitioned Parquet sink
â”‚   â”‚   â”œâ”€â”€ ds_silver_parquet.json  # Cleansed Silver output
â”‚   â”‚   â””â”€â”€ ds_gold_sql.json        # Azure SQL star schema tables
â”‚   â”œâ”€â”€ linkedService/
â”‚   â”‚   â”œâ”€â”€ ls_adls_gen2.json       # Data Lake Storage (Managed Identity)
â”‚   â”‚   â”œâ”€â”€ ls_azure_sql.json       # Gold DB (Key Vault secret)
â”‚   â”‚   â””â”€â”€ ls_key_vault.json       # Centralized secret management
â”‚   â””â”€â”€ pipeline/
â”‚       â”œâ”€â”€ PL_Ingest_Bronze.json   # Lookup â†’ ForEach â†’ Copy (CSV â†’ Parquet)
â”‚       â”œâ”€â”€ PL_Transform_Silver.json # Execute JDE decoder + SCD2 data flows
â”‚       â”œâ”€â”€ PL_Load_Gold.json       # Point-in-time fact table loading
â”‚       â””â”€â”€ PL_Master.json          # Orchestrator with failure alerting
â”œâ”€â”€ config/
â”‚   â””â”€â”€ source_config.json          # Metadata-driven ingestion configuration
â”œâ”€â”€ data/                           # Generated CSVs (gitignored)
â”œâ”€â”€ sql_scripts/
â”‚   â”œâ”€â”€ create_schemas.sql          # Bronze / Silver / Gold schema DDL
â”‚   â”œâ”€â”€ dim_customer.sql            # SCD2 customer dimension
â”‚   â”œâ”€â”€ dim_date.sql                # 20-year calendar dimension
â”‚   â””â”€â”€ fact_sales.sql              # Star schema fact table
â””â”€â”€ src/
    â””â”€â”€ python/
        â”œâ”€â”€ generate_jde_data.py    # Synthetic JDE data generator
        â””â”€â”€ requirements.txt        # pandas, faker
```

---

## ðŸš€ Getting Started

### Prerequisites

- **Azure Subscription** with Data Factory, Storage Account (HNS enabled), SQL Database, and Key Vault
- **Python 3.11+** for data generation
- **Azure CLI** or **Azure Storage Explorer** for file uploads

### Step 1: Generate Synthetic Data

```bash
cd data-warehouse-migration
pip install -r src/python/requirements.txt
python src/python/generate_jde_data.py
```

This creates `data/F0101.csv` (50 customers) and `data/F4211.csv` (200 sales orders) with JDE-formatted fields.

### Step 2: Upload to ADLS Gen2

Using Azure Storage Explorer or CLI, upload the CSVs to your Data Lake container:

```
datalake/01-bronze/landing/F0101.csv
datalake/01-bronze/landing/F4211.csv
```

Also upload `config/source_config.json` to `datalake/config/source_config.json`.

### Step 3: Create Gold Layer Tables

Execute the SQL scripts against your Azure SQL Database in order:

```bash
sqlcmd -S your-server.database.windows.net -d sql-dw-gold -U admin -P 'password' \
  -i sql_scripts/create_schemas.sql \
  -i sql_scripts/dim_date.sql \
  -i sql_scripts/dim_customer.sql \
  -i sql_scripts/fact_sales.sql
```

### Step 4: Connect ADF to GitHub

1. Open **ADF Studio** â†’ **Manage** â†’ **Git Configuration**  
2. Select **GitHub**, point to this repository  
3. Set **Collaboration Branch** = `main`, **Root Folder** = `/adf/`  
4. ADF will auto-discover all pipelines, datasets, and data flows

### Step 5: Run the Pipeline

Trigger `PL_Master` in ADF Studio. It executes sequentially:

1. **Bronze** â€” Ingests CSVs â†’ Parquet (partitioned)
2. **Silver** â€” Decodes Julian dates + decimal normalization + SCD2
3. **Gold** â€” Loads star schema with point-in-time dimension joins

---

## ðŸ’° Cost Optimization Strategy

| Resource | Configuration | Monthly Cost |
|----------|--------------|-------------|
| Azure SQL Database | Basic Tier (5 DTU / 2GB) | ~$4.99 |
| ADF Data Flows | TTL = 0 (no idle billing) | Pay per use |
| ADLS Gen2 | Cool tier lifecycle policy | ~$0.01/GB |
| Key Vault | Standard tier | ~$0.03/10K ops |
| **Total** | | **< $15/month** |

> **FinOps Note:** Debug sessions are limited to 60-minute timeouts. Spark clusters spin up on-demand and shut down immediately after execution (TTL=0).

---

## ðŸ”„ SCD Type 2 â€” How It Works

```mermaid
sequenceDiagram
    participant New as New F0101 Data
    participant Hash as SHA256 Hasher
    participant Lookup as Business Key Lookup
    participant Split as Conditional Split
    participant SQL as Gold.Dim_Customer

    New->>Hash: Hash business columns
    Hash->>Lookup: Match on CustomerID
    Lookup->>Split: Compare hashes
    
    Split->>SQL: NEW â†’ Insert (IsActive=1)
    Split->>SQL: CHANGED â†’ Update old (IsActive=0)
    Split->>SQL: CHANGED â†’ Insert new (IsActive=1)
    Split-->>Split: NO CHANGE â†’ Discard
```

---

## ðŸ“„ License

This project is open-source under the [MIT License](LICENSE).

---

<p align="center">
  <i>Built as a portfolio demonstration of enterprise-grade Azure Data Engineering capabilities.</i>
</p>
